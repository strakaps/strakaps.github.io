<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Peter&#39;s Homepage on Peter&#39;s Homepage</title>
    <link>https://strakaps.github.io/</link>
    <description>Recent content in Peter&#39;s Homepage on Peter&#39;s Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +1100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Add Interactions to Regularized Regression</title>
      <link>https://strakaps.github.io/post/glinternet/</link>
      <pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/glinternet/</guid>
      <description>


&lt;div id=&#34;lasso-glinternet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lasso &amp;amp; glinternet&lt;/h3&gt;
&lt;p&gt;Every Data Scientist and her dog know linear and logistic regression. The majority will probably also know that these models have regularized versions, which increase predictive performance by reducing variance (at the cost of a small increase in bias). Choosing L1-regularization (Lasso) even gets you variable selection for free. The theory behind these models is covered expertly in &lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/&#34;&gt;&lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;&lt;/a&gt; (for an easier version, see &lt;a href=&#34;https://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;&lt;em&gt;An Introduction to Statistical Learning&lt;/em&gt;&lt;/a&gt;), and implemented nicely in the packages &lt;code&gt;glmnet&lt;/code&gt; for R and &lt;a href=&#34;http://scikit-learn.org/stable/modules/linear_model.html#lasso&#34;&gt;&lt;code&gt;scikitlearn&lt;/code&gt;&lt;/a&gt; for Python.&lt;/p&gt;
&lt;p&gt;Few people, however, have heard about &lt;code&gt;glinternet&lt;/code&gt;, which is a powerful extension of the Lasso model:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;glinternet&lt;/code&gt; adds pairwise variable interactions into the Lasso model, and selects these automatically.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s a &lt;strong&gt;linear model&lt;/strong&gt;, and thus easily interpretable. Its computational complexity is the same as penalized regression: &lt;span class=&#34;math inline&#34;&gt;\(\mathcal O(Nm)\)&lt;/span&gt; where there are &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; variables&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Thus it handles problems of the same size as (deep) neural networks, potentially with tens of thousands of variables. It should be stressed here that for a thousand variables there are almost 500,000 pairwise interactions that glinternet checks! For structured data, logistic regression is a very useful benchmark model, which can be &lt;a href=&#34;https://twitter.com/ShalitUri/status/1009534668880928769&#34;&gt;on par with sophisticated deep learning models&lt;/a&gt; even for huge datasets. The glinternet model is more flexible than logistic regression, and is just as interpretable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-glinternet-tutorial&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A glinternet tutorial&lt;/h1&gt;
&lt;p&gt;Let’s look at the &lt;code&gt;car90&lt;/code&gt; dataset in the &lt;code&gt;rpart&lt;/code&gt; package. It has 25 categorical and 8 continuous predictors, and a continuous outcome (Price). We fit a glinternet model to it, which is a linear model containing all possible pairwise interactions.&lt;/p&gt;
&lt;div id=&#34;interactions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interactions&lt;/h3&gt;
&lt;p&gt;A linear model &lt;em&gt;without&lt;/em&gt; interactions is usually written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \sum_k \beta_k X_k + \varepsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept, &lt;span class=&#34;math inline&#34;&gt;\(X_k\)&lt;/span&gt; are predictor variables, &lt;span class=&#34;math inline&#34;&gt;\(\beta_k\)&lt;/span&gt; the coefficients and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon\)&lt;/span&gt; a model error. We will use categorical variables, so we need to group the variables and coefficients with their categories:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \sum_{i=1}^m \sum_{l=1}^{L_i} \beta_{i,l} X_{i,l} + \varepsilon\]&lt;/span&gt; where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is the number of predictors &lt;span class=&#34;math inline&#34;&gt;\(X_1, \ldots, X_m\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(L_i\)&lt;/span&gt; is the number of levels of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; is categorical, and &lt;code&gt;1&lt;/code&gt; otherwise&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\((X_{i,1}, \ldots, X_{i,L_i})\)&lt;/span&gt; are 0-1 dummy variables representing the categories of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; (one-hot encoded)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A pairwise interaction model contains all possible pairwise products of the predictors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \sum_{l=1}^{L_i} \sum_{i=1}^m \beta_{i,l}, X_{i,l} 
+ \sum_{1 \le i&amp;lt;j \le m} \sum_{l=1}^{L_i} \sum_{k=1}^{L_j} \theta_{i,j,l,k} X_{i,l} X_{j,k}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Interactions, e.g. between variables &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt;, occur when the effect of &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; will vary depending on the level (or value) of &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;. In that case, some of the values &lt;span class=&#34;math inline&#34;&gt;\(\theta_{i,j, \cdot, \cdot}\)&lt;/span&gt; will be non-zero.&lt;/p&gt;
&lt;div id=&#34;selection-of-variables-and-interactions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Selection of variables and interactions&lt;/h4&gt;
&lt;p&gt;The L1 regularization is known as the lasso and produces sparsity. glinternet uses a &lt;em&gt;group&lt;/em&gt; lasso for the variables and variable interactions, which introduces the following &lt;em&gt;strong hierarchy&lt;/em&gt;: An interaction between &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; can only be picked by the model if both &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; are also picked. In other words, interactions between two predictors are not considered unless both predictors have non-zero coefficients in the model. The details are in the original &lt;a href=&#34;https://doi.org/10.1080/10618600.2014.938812&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-glinternet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitting glinternet&lt;/h3&gt;
&lt;p&gt;Unfortunately, &lt;code&gt;glinternet&lt;/code&gt;’s “API” is not super user friendly… the first challenge is to bring the data into a form that glinternet accepts: The categorical variables need to be (somewhat un-R-like) coded as integers starting from 0. We also need to construct a &lt;code&gt;numLevels&lt;/code&gt; vector containing the number of levels &lt;span class=&#34;math inline&#34;&gt;\(L_1, \ldots, L_m\)&lt;/span&gt; in each column if the column is categorical, and &lt;code&gt;1&lt;/code&gt; else:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

# Model2 contains model names, which aren&amp;#39;t useful here
df &amp;lt;- rpart::car90 %&amp;gt;% select(-Model2)

# drop rows with empty outcomes
df &amp;lt;- df[!is.na(df$Price), ]
y &amp;lt;- df$Price
df &amp;lt;- df %&amp;gt;% select(-Price)

# impute the median for the continuous variables
i_num &amp;lt;- sapply(df, is.numeric)
df[, i_num] &amp;lt;- apply(df[, i_num], 2, function(x) ifelse(is.na(x), median(x, na.rm=T), x))

# impute empty categories
df[, !i_num] &amp;lt;- apply(df[, !i_num], 2, function(x) {
  x[x==&amp;quot;&amp;quot;] &amp;lt;- &amp;quot;empty&amp;quot;
  x[is.na(x)] &amp;lt;- &amp;quot;missing&amp;quot;
  x
})

# get the numLevels vector containing the number of categories
X &amp;lt;- df
X[, !i_num] &amp;lt;- apply(X[, !i_num], 2, factor) %&amp;gt;% as.data.frame()
numLevels &amp;lt;- X %&amp;gt;% sapply(nlevels)
numLevels[numLevels==0] &amp;lt;- 1

# make the categorical variables take integer values starting from 0
X[, !i_num] &amp;lt;- apply(X[, !i_num], 2, function(col) as.integer(as.factor(col)) - 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A single glinternet model is fitted with the function &lt;code&gt;glinternet&lt;/code&gt;; here, we directly fit a 10-fold Cross-Validated model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glinternet)
set.seed(1001)
cv_fit &amp;lt;- glinternet.cv(X, y, numLevels)
plot(cv_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/post/2018-10-13-glinternet_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see the MSE plotted against the decreasing values of regularization parameter &lt;code&gt;lambda&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i_1Std &amp;lt;- which(cv_fit$lambdaHat1Std == cv_fit$lambda)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is common &lt;em&gt;not&lt;/em&gt; to pick the lambda corresponding to the lowest estimate of cross-validation error, but to err on the side of more regularization. So we are happy with a larger lambda with CV error estimate below the minimum plus one standard deviation. This lambda has index &lt;code&gt;i_1Std&lt;/code&gt; = 22. Its coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_{i,l}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_{i,j,l,k}\)&lt;/span&gt; can be extracted by running&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs &amp;lt;- coef(cv_fit$glinternetFit)[[i_1Std]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;coefs&lt;/code&gt; is a list of 4 items: &lt;code&gt;mainEffects&lt;/code&gt;, &lt;code&gt;mainEffectsCoef&lt;/code&gt;, &lt;code&gt;interactions&lt;/code&gt; and &lt;code&gt;interactionsCoef&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s look at the part without interactions first: The main effects identified are&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs$mainEffects&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $cat
## [1] 3
## 
## $cont
## [1]  5  7  9 12 20 22 11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;which are the indices of the categorical variables and the continuous variables, in order as they occur in the data. We get the indices via&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;idx_num &amp;lt;- (1:length(i_num))[i_num]
idx_cat &amp;lt;- (1:length(i_num))[!i_num]
names(numLevels)[idx_cat[coefs$mainEffects$cat]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Rim&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(numLevels)[idx_num[coefs$mainEffects$cont]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Frt.Leg.Room&amp;quot; &amp;quot;Gear.Ratio&amp;quot;   &amp;quot;HP&amp;quot;           &amp;quot;Length&amp;quot;      
## [5] &amp;quot;Tank&amp;quot;         &amp;quot;Weight&amp;quot;       &amp;quot;Height&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficients for these variables are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs$mainEffectsCoef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $cat
## $cat[[1]]
## [1]    357.6320  -8221.4669   6589.1535 -36595.3391    118.0856   -920.4584
## 
## 
## $cont
## $cont[[1]]
## [1] 35.48628
## 
## $cont[[2]]
## [1] 314.3327
## 
## $cont[[3]]
## [1] -477.2347
## 
## $cont[[4]]
## [1] 9.059015
## 
## $cont[[5]]
## [1] 507.8227
## 
## $cont[[6]]
## [1] 1.366189
## 
## $cont[[7]]
## [1] 104.6502&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(I would have expected all of these to be positive; one should check why &lt;code&gt;HP&lt;/code&gt; has a negative coefficient.)&lt;/p&gt;
&lt;p&gt;Now for the interaction pairs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs$interactions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $catcat
## NULL
## 
## $contcont
##      [,1] [,2]
## [1,]    5    9
## [2,]   11   20
## 
## $catcont
##      [,1] [,2]
## [1,]    3    5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is no interaction between the categorical variables (unsurprising because we only identified one. If there was an interaction pair, we would access it via &lt;code&gt;coefs$interactionsCoef$catcat&lt;/code&gt;, giving a list of matrices, one matrix of dimension &lt;span class=&#34;math inline&#34;&gt;\(L_i \times L_j\)&lt;/span&gt; for each pair.)&lt;/li&gt;
&lt;li&gt;Two pairs of continuous variables have interactions: (5,9) and (11,20), which are (Frt.Leg.Room, HP) and (Height, Tank). The coefficients for these interactions are&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs$interactionsCoef$contcont&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] 13.66277
## 
## [[2]]
## [1] -6.634724&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;There is one interaction between categorical variable 3 and continuous variable 5, which are Rim and Frt.Leg.Room. The coefficients for this interaction are:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs$interactionsCoef$catcont&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] -164.0751   37.7502 -324.6745  741.9752 -158.0835 -132.8923&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our simple interaction network looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/post/2018-10-13-glinternet_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model has a root mean squared error (RMSE) on validation data of&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(cv_fit$cvErr[[i_1Std]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3411.904&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-to-linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compare to linear regression&lt;/h3&gt;
&lt;p&gt;To quantify how much interactions have contributed to predictive power, we now fit a glmnet model. The best CV mean-squared error for the glmnet model is higher than the mean-squared error for glinternet:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df$Price &amp;lt;- y
X &amp;lt;- model.matrix(Price ~ . - 1, df)
library(glmnet)
cv_glmnet &amp;lt;- cv.glmnet(X,y)
sqrt(min(cv_glmnet$cvm))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4238.202&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;take-home-message&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Take-home message&lt;/h2&gt;
&lt;p&gt;When you’re using logistic or linear regression with Lasso regularization as a baseline, and your next step is to see if adding interactions improve predictive power: try glinternet!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1080/10618600.2014.938812&#34;&gt;Learning Interactions via Hierarchical Group-Lasso Regularization&lt;/a&gt;. Michael Lim &amp;amp; Trevor Hastie&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Though for this to be true, I believe you have to limit the “interaction search space” via the &lt;code&gt;screenLimit&lt;/code&gt; parameter.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Variable Order Fractional Fokker-Planck Equations derived from Continuous Time Random Walks</title>
      <link>https://strakaps.github.io/publication/2018-voffpe/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/publication/2018-voffpe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Identification of pollutant source for super-diffusion in aquifers and rivers with bounded domains</title>
      <link>https://strakaps.github.io/publication/2018-wrr/</link>
      <pubDate>Tue, 31 Jul 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/publication/2018-wrr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Overcoming the data drought: exploring general practice in Australia by network analysis of big data</title>
      <link>https://strakaps.github.io/publication/2018-mja/</link>
      <pubDate>Sun, 29 Jul 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/publication/2018-mja/</guid>
      <description>&lt;p&gt;See my &lt;a href=&#34;https://strakaps.github.io/post/2018-07-mja-paper/&#34;&gt;blog post for this publication&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Networks in Healthcare</title>
      <link>https://strakaps.github.io/post/2018-07-mja-paper/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/2018-07-mja-paper/</guid>
      <description>

&lt;p&gt;&lt;em&gt;See also the
&lt;a href=&#34;https://www.mja.com.au/podcast/209/2/mja-podcasts-2018-episode-57-big-gp-data-research-prof-louisa-jorm-and-dr-michael&#34; target=&#34;_blank&#34;&gt;MJA podcast episode&lt;/a&gt;
accompanying this article.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Our joint work
(&lt;a href=&#34;https://cbdrh.med.unsw.edu.au/&#34; target=&#34;_blank&#34;&gt;UNSW CBDRH&lt;/a&gt; and
&lt;a href=&#34;https://www.maths.unsw.edu.au/about/Statistics&#34; target=&#34;_blank&#34;&gt;Statistics&lt;/a&gt;)
which analyses Australian patient claim data using big network algorithms
is now available on the
&lt;a href=&#34;https://www.mja.com.au/journal/2018/209/2/overcoming-data-drought-exploring-general-practice-australia-network-analysis&#34; target=&#34;_blank&#34;&gt;MJA website&lt;/a&gt;.
We have processed
&lt;a href=&#34;http://www.mbsonline.gov.au/internet/mbsonline/publishing.nsf/Content/Home&#34; target=&#34;_blank&#34;&gt;MBS&lt;/a&gt;
claims data of 10% of Australians over the years 1994-2014, trying to shed light
on the following research questions:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What is the patient sharing behaviour of general practitioners (GPs): are there any
meaningful clusters (called &amp;ldquo;Provider Practice Communities, PPC&amp;rdquo;)
of GPs which collaborate and share patients? How have these
clusters changed in the course of 20 years?&lt;/li&gt;
&lt;li&gt;How does Continuity of Care (CoC&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;) differ between large and small
PPCs?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Datasets such as provided by
&lt;a href=&#34;http://sydney.edu.au/medicine/Public Health/research/beach.php&#34; target=&#34;_blank&#34;&gt;BEACH&lt;/a&gt;
(Bettering the Evaluation And Care of Health, USYD)
would be ideal to tackle these questions, but were
&lt;a href=&#34;http://sydney.edu.au/medicine/fmrc/media/FMRC-closure-2016-06.php&#34; target=&#34;_blank&#34;&gt;discontinued&lt;/a&gt;
due to lack of funding, and it may take several years until Australia&amp;rsquo;s
&lt;a href=&#34;https://www.myhealthrecord.gov.au/&#34; target=&#34;_blank&#34;&gt;My Health Record&lt;/a&gt;
is adopted by enough Australians to generate useful datasets.&lt;/p&gt;

&lt;p&gt;Question 1 really is about the structure of information sharing in the Australian
health system. We believe we&amp;rsquo;ve found evidence of information sharing within clusters
of GPs (PPCs) which resemble the typical sizes of practices of GPs.
CoC is a proxy for the sharing of important patient information sharing,
which is believed to lead to better health outcomes;
but information sharing can also mean that GPs
have similar behaviour when referring to specialists or pathology
services, among which there may be low-value services such as unwarranted
surgical interventions and blood tests.&lt;/p&gt;

&lt;p&gt;Based on the PPCs which we have identified,
&lt;strong&gt;it is now possible to discover unwarranted variation of good and bad outcomes
on the level of PPCs (practices)&lt;/strong&gt;.
There are many interesting questions that can be tackled now, and
as a first step into this direction, we redefined &amp;ldquo;Continuity of Care&amp;rdquo;
on the PPC level, creating an indicator which is high if a patient is loyal
to the same practice, rather than the same GP.
It seems that the larger a practice, the less loyal patients are to individual
GPs; but if loyalty to the same practice is measured, small and large
practices seem to fare alike.&lt;/p&gt;

&lt;h1 id=&#34;big-networks&#34;&gt;Big Networks&lt;/h1&gt;

&lt;p&gt;The usual approach in network clustering
(which was seemingly taken by &lt;em&gt;all&lt;/em&gt; previous papers on this topic in the
medical literature)
is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Create a network of GPs only&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, where patients only appear as weights
on the edges
(i.e. &lt;a href=&#34;https://en.wikipedia.org/wiki/Bipartite_network_projection&#34; target=&#34;_blank&#34;&gt;project from bipartite to unipartite&lt;/a&gt;):
&lt;img src=&#34;https://strakaps.github.io/img/Bipartite_network_projection.png&#34; alt=&#34;projection&#34; /&gt;
&lt;small&gt; (source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bipartite_network_projection#/media/File:Bipartite_network_projection.png&#34; target=&#34;_blank&#34;&gt;Wikipedia&lt;/a&gt;) &lt;/small&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Cluster the GP nodes by maximizing&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Modularity_(networks)&#34; target=&#34;_blank&#34;&gt;modularity&lt;/a&gt;:
&lt;img src=&#34;https://strakaps.github.io/img/modularity.jpg&#34; width=&#34;300&#34;&gt;
&lt;small&gt; (source: &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1053811916306449&#34; target=&#34;_blank&#34;&gt;sciencedirect&lt;/a&gt;) &lt;/small&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The main flaws with this approach are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;There is no good way to construct edge weights between GPs.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;The
&lt;a href=&#34;https://en.wikipedia.org/wiki/Modularity_(networks)#Resolution_limit&#34; target=&#34;_blank&#34;&gt;resolution limit problem&lt;/a&gt;
of modularity:
The minimum size of discoverable communities grows with the number of nodes
$N$ in the network, roughly like $\sqrt N$. Since we study thousands of GPs,
this problem is prohibitive.&lt;/li&gt;
&lt;li&gt;Modularity assumes that all block structure is assortative, which can be
dangerous; moreover even
&lt;a href=&#34;https://journals.aps.org/pre/abstract/10.1103/PhysRevE.70.025101&#34; target=&#34;_blank&#34;&gt;completely random networks can have high modularity&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Using the Python package
&lt;a href=&#34;http://dx.doi.org/10.6084/m9.figshare.1164194&#34; target=&#34;_blank&#34;&gt;graph-tool&lt;/a&gt;,
we were able to:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Avoid the bipartite projection, even though this meant dealing with millions
of nodes (patients and doctors) instead of thousands (doctors only).&lt;/li&gt;
&lt;li&gt;Fit a
&lt;a href=&#34;https://journals.aps.org/prx/abstract/10.1103/PhysRevX.4.011047&#34; target=&#34;_blank&#34;&gt;hierarchical stochastic blockmodel&lt;/a&gt;,
which allows detecting much smaller communities.
Moreover, the block structure respected bipartiteness, meaning that each block
was made up of either doctors only or patients only.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;computations&#34;&gt;Computations&lt;/h1&gt;

&lt;p&gt;The main challenges were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Installing graph-tool:&lt;/strong&gt; pip and conda are not an option, you have to compile
it from source, or install the native Linux / MacOS X packages.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RAM usage:&lt;/strong&gt; graph-tool needed up to 64GB of RAM.
We&amp;rsquo;ve used the
&lt;a href=&#34;https://www.hpc.science.unsw.edu.au/&#34; target=&#34;_blank&#34;&gt;Katana computational cluster&lt;/a&gt;
at UNSW and got excellent support along the way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Numerous runs:&lt;/strong&gt; The hierarchical blockmodelling algorithm converges to
a different solution every time; to be somewhat confident to have found a
good minimum of the description length, we needed 4 runs of up to 6 hours
each, for each of 5 regions.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;github-repository&#34;&gt;GitHub Repository&lt;/h1&gt;

&lt;p&gt;The full Python code used for this project, as well as more details on the
blockmodelling approach, can be found on the
&lt;a href=&#34;https://github.com/CBDRH/GP-networks&#34; target=&#34;_blank&#34;&gt;CBDRH GitHub page&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Using hierarchical stochastic blockmodelling, it is possible to use data
which is routinely collected by GPs to monitor characteristics of Australian
general practices. Future research will clarify how these characteristics affect
patient care.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;A patient typically seeing the same GP, or GP clinic, experiences high CoC:
$${\rm CoC} = \frac{\max \left\lbrace n_k \right \rbrace}{\sum_k n_k}$$
where $n_k$ are the numbers of visits made to different PPCs.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:3&#34;&gt;&lt;p&gt;In the graph shown, suppose X are the patients and Y the doctors; then the
Y-projection is a network of doctors only, with edge weights computed from
the number of shared patients.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:3&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:4&#34;&gt;&lt;p&gt;The picture shows the adjacency matrix, where each row and column
corresponds to one node, and a black dot means there is an edge.
The red squares represent a grouping of nodes.
Modularity, in a nutshell, is the gain in the
number of within-group edges achieved by the grouping,
compared to the expected number if all edges were
reassigned at random.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:4&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;

&lt;li id=&#34;fn:2&#34;&gt;&lt;p&gt;If you go by the number of shared patients,
this discards all patients which have
only seen one GP, and makes it impossible to find single GPs.
If you go by &lt;a href=&#34;https://en.wikipedia.org/wiki/Jaccard_index&#34; target=&#34;_blank&#34;&gt;Jaccard index&lt;/a&gt;,
this leads to weights between 0 and 1, which is not useful for most
community detection algorithms.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>NUS-MIT Datathon</title>
      <link>https://strakaps.github.io/post/nus-mit-datathon/</link>
      <pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/nus-mit-datathon/</guid>
      <description>

&lt;p&gt;Last week I had the privilege to participate in the
&lt;a href=&#34;http://www.nus-datathon.com/&#34; target=&#34;_blank&#34;&gt;NUS-NUH-MIT DATATHON&lt;/a&gt;
and Workshop on applications of AI in healthcare with the
&lt;a href=&#34;https://cbdrh.med.unsw.edu.au/&#34; target=&#34;_blank&#34;&gt;UNSW Centre for Big Data Research in Health (CBDRH)&lt;/a&gt;
team (Tim Churches, Mark Hanly, Oisin Fitzgerald and Oluwadamisola Sotade).&lt;/p&gt;

&lt;h1 id=&#34;thu-fri-workshop-talks&#34;&gt;Thu &amp;amp; Fri: Workshop &amp;amp; Talks&lt;/h1&gt;

&lt;p&gt;In the workshop
&amp;ldquo;Deploying AI Solutions in Real Clinical Practices&amp;rdquo; by
Dr Ngiam Kee Yuan (CTO, NUHS)
we discussed&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The large NUHS (National University Health System) databases and their storage structure&lt;/li&gt;
&lt;li&gt;Data security and ownership&lt;/li&gt;
&lt;li&gt;Applications for access to data&lt;/li&gt;
&lt;li&gt;The always changing standards of diagnosis codes (ICD9, ICD10, SNOMED, &amp;hellip;)
and the problem of matching doctors diagnoses to these codes. In another talk,
Hector Yee
(&lt;a href=&#34;https://twitter.com/eigenhector&#34; target=&#34;_blank&#34;&gt;@eigenhector&lt;/a&gt;)
shared some vice ideas on how to use word embeddings as a
tool for matching hand-typed diagnoses to ICD10 codes.&lt;/li&gt;
&lt;li&gt;The importance of getting doctors to trust AI algorithms.
Hector presented an interesting approach which iterates between an AI
algorithm proposing codes and a doctor validating these.
The goal is of course to &lt;em&gt;augment&lt;/em&gt; the doctors, not to &lt;em&gt;replace&lt;/em&gt; them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;fri-sun-datathon&#34;&gt;Fri - Sun: Datathon&lt;/h1&gt;

&lt;h2 id=&#34;friday-6pm-pitching-topics-forming-teams&#34;&gt;Friday 6pm: Pitching Topics &amp;amp; Forming Teams&lt;/h2&gt;

&lt;p&gt;23 clinicians pitch topics. We were most interested in a topic presented by
Dr Lui Pak Ling (Hematology) and Dr Stella Ang (Anesthesiology) at NUH:&lt;/p&gt;

&lt;p&gt;Currently-used statistical models predicting the mortality risk of Intensive Care Unit (ICU)
patients assume that the importance of prognostic factors does not vary&lt;/p&gt;

&lt;p&gt;Research Database, a large multi-centre critical care database made available by Philips Healthcare in partnership with the MIT Laboratory for Computational Physiology, they investigated whether the importance of prognostic factors varied in the course of each patient’s stay in ICU. Currently-used predictive models assume that they don’t vary.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Currently, the mortality risk of ICU (intensive care unit) patients is assessed
based on a collection of variables at the time they are presented to the ICU
(the so-called
&lt;a href=&#34;https://en.wikipedia.org/wiki/APACHE_II&#34; target=&#34;_blank&#34;&gt;Apache score&lt;/a&gt;). This score is assumed
to be the same &lt;em&gt;during the whole course of stay&lt;/em&gt; in the ICU.
Is this assumption appropriate, or does the importance of some prognostic variables
in fact change over time?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We joined forces with &lt;a href=&#34;https://www.mornin-feng.com/siqi&#34; target=&#34;_blank&#34;&gt;Siqi Liu&lt;/a&gt;,
a PhD student at Saw Swee Hock School of Public Health and
a member of the organising team for the Datathon who represented our team,
and &lt;a href=&#34;https://www.linkedin.com/in/peng-shen-137568154/&#34; target=&#34;_blank&#34;&gt;Peng Shen&lt;/a&gt;
(MOH Holdings Pte Ltd).&lt;/p&gt;

&lt;h2 id=&#34;all-of-saturday-extracting-data&#34;&gt;All of Saturday: Extracting Data&lt;/h2&gt;

&lt;p&gt;We used the
&lt;a href=&#34;https://eicu-crd.mit.edu/about/eicu/&#34; target=&#34;_blank&#34;&gt;eICU Collaborative Research Database&lt;/a&gt;.
The organisers have made the data available via Google&amp;rsquo;s BigQuery, which was
very quick and (almost) 100% reliable.
Working in pairs we extracted and cleaned various tables of variables
that we used as predictors:
comorbidity history, lab results, vitalperiodic and vitalaperiodic, coma score,
infusion drugs, dialysis, blood culture, mechanical ventilation, &amp;hellip;&lt;/p&gt;

&lt;p&gt;We queried the database using&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the &lt;a href=&#34;https://cloud.google.com/bigquery/quickstart-web-ui&#34; target=&#34;_blank&#34;&gt;BigQuery&lt;/a&gt; web interface,&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://pandas-gbq.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34;&gt;Python API&lt;/a&gt; and colab notebooks, or&lt;/li&gt;
&lt;li&gt;the &lt;a href=&#34;https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html&#34; target=&#34;_blank&#34;&gt;dbplyr&lt;/a&gt; package in R.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sunday-morning-more-wrangling&#34;&gt;Sunday morning: More wrangling&lt;/h2&gt;

&lt;p&gt;Joining all the extracted tables into one big table.
The final SQL query was 306 lines long and extracted
of 161,988 rows (a cohort of sepsis patients)
and 93 transformed and cleaned variables (categorical and continuous)
from tables with 224 million rows with 4-51 columns.&lt;/p&gt;

&lt;h2 id=&#34;sunday-12pm-3pm&#34;&gt;Sunday 12pm-3pm:&lt;/h2&gt;

&lt;h4 id=&#34;model-fitting&#34;&gt;Model fitting&lt;/h4&gt;

&lt;p&gt;As a baseline, we used logistic regression, with L1 penalty for variable
selection (using &lt;a href=&#34;https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html&#34; target=&#34;_blank&#34;&gt;glmnet&lt;/a&gt;
in R). Virtually all rows had missing data,
so we need to impute by median.
This didn&amp;rsquo;t seem to work out very well, as e.g. the variables picked at after
2 days in ICU had almost nothing in common with the variables picked for
3 or 4 days in ICU.&lt;/p&gt;

&lt;p&gt;Gradient Boosting (using &lt;a href=&#34;https://tech.yandex.com/catboost/&#34; target=&#34;_blank&#34;&gt;catboost&lt;/a&gt;
 in R) seemed to work stably with the
irregularly spaced, high-dimensional data with many missing values.
In our preliminary analysis, it seems that &amp;ldquo;age&amp;rdquo; may increase in importance
and &amp;ldquo;SaO2&amp;rdquo; (oxygen saturation) may decrease in importance.&lt;/p&gt;

&lt;h4 id=&#34;presentation-writing&#34;&gt;Presentation writing:&lt;/h4&gt;

&lt;p&gt;Teams were judged on a three minute presentation only, and the time limit
was strictly enforced (our MC made the audience start clapping after
exactly 180 seconds). So the presentation had to be practiced to near
perfection, which Dami and Siqi succeeded in doing:
The judges placed us first in the track &amp;ldquo;AI for critical care&amp;rdquo; 🎉🎉🎉&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/img/datathon2.jpg&#34; alt=&#34;grinners&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;work-to-be-continued&#34;&gt;Work to be continued&lt;/h1&gt;

&lt;p&gt;So far, we have fitted individual models to each point in time
(day1, day2, &amp;hellip;, day5). This is of course not the ideal way to deal with
a longitudinal signal. One ought to try&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf&#34; target=&#34;_blank&#34;&gt;Survival analysis with time-dependent coefficients&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rd.springer.com/article/10.1007/s10994-016-5597-1&#34; target=&#34;_blank&#34;&gt;Boosted multivariate trees for longitudinal data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;the-takeaway&#34;&gt;The takeaway&lt;/h1&gt;

&lt;p&gt;Our team is walking away with a preliminary analysis on a cleaned dataset which is
likely to result in a publication, and 2 domain experts and 1 other data
scientist who are invested in continuing our work. I would say these two
days were incredibly well spent!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD internship in Machine Learning</title>
      <link>https://strakaps.github.io/post/phd-internship-in-machine-learning/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/phd-internship-in-machine-learning/</guid>
      <description>&lt;p&gt;In the internship, you&amp;rsquo;ll explore the latest machine learning methods such as
tree ensemble methods, graphical models and neural networks and compare their
performance to what is the current industry standard. A great opportunity to
improve your machine learning skills and create valuable insights for the
credit risk industry.&lt;/p&gt;

&lt;p&gt;Applications close 20 June, 2018.&lt;/p&gt;

&lt;p&gt;Apply here: &lt;a href=&#34;https://aprintern.org.au/2018/05/21/int-0431/&#34; target=&#34;_blank&#34;&gt;https://aprintern.org.au/2018/05/21/int-0431/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Networks of Health Providers</title>
      <link>https://strakaps.github.io/project/gp-networks/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/project/gp-networks/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;What is the network structure of healthcare providers and their
practices/clinics?&lt;/li&gt;
&lt;li&gt;What is their patient sharing pattern?&lt;/li&gt;
&lt;li&gt;Does sharing of patient information between providers lead to better health
outcomes?&lt;/li&gt;
&lt;li&gt;Do practice size and patient sharing behaviour influence the quality of
healthcare?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Australian Medicare claims data contains
data&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; on tens of thousands of healthcare providers and millions of
patients, such as patient characteristics
(age, sex, &amp;hellip;)
and provider characteristics
(specialty, type and frequency of services delivered, bulk-billing behaviour,&lt;br /&gt;
&amp;hellip;).&lt;/p&gt;

&lt;p&gt;The natural structure of these data is a network, where
providers &amp;amp; patients are connected whenever a service is provided.
The structure of this network contains crucial information on the flow of
information and the variation of healthcare quality, and thus allows to
tackle questions of national importance such as the above.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;de-identified, of course.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R package CTRE</title>
      <link>https://strakaps.github.io/project/ctre/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/project/ctre/</guid>
      <description></description>
    </item>
    
    <item>
      <title>R package CTRE: thresholding bursty time series</title>
      <link>https://strakaps.github.io/post/ctre-package/</link>
      <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/ctre-package/</guid>
      <description>


&lt;p&gt;The new &lt;a href=&#34;https://strakaps.github.io/CTRE/&#34;&gt;R package &lt;code&gt;CTRE&lt;/code&gt;&lt;/a&gt; is now available on CRAN. “CTRE” stands for “Continuous Time Random Exceedances”, which is a model for extreme values of bursty time series. The theory is desribed in another paper (&lt;a href=&#34;https://strakaps.github.io/bursty-POT/&#34;&gt;html&lt;/a&gt;|&lt;a href=&#34;https://arxiv.org/pdf/1802.05218.pdf&#34;&gt;pdf&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(CTRE)
flares_ctre &amp;lt;- ctre(flares)
plot(flares_ctre, log = &amp;quot;y&amp;quot;, main = &amp;quot;Solar Flares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/post/ctre-package_files/figure-html/data-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above time series are measurements of solar flare magnitudes by NASA&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. Which observations are considered extreme is defined by a variable threshold &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; (dashed line, &lt;span class=&#34;math inline&#34;&gt;\(&amp;lt; 5\%\)&lt;/span&gt; of observations). The exceedances of this threshold (red) follow (asymptotically for high &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt;) a &lt;a href=&#34;https://en.wikipedia.org/wiki/Generalized_Pareto_distribution&#34;&gt;Generalized Pareto distribution&lt;/a&gt;. This type of modelling of extremes is standard in Extreme Value Theory, and known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Extreme_value_theory#Data_analysis&#34;&gt;Peaks Over Threshold&lt;/a&gt; (POT) method. To date, &lt;a href=&#34;https://cran.r-project.org/web/views/ExtremeValue.html&#34;&gt;&lt;em&gt;18 R packages&lt;/em&gt;&lt;/a&gt; have been developed for POT!&lt;/p&gt;
&lt;div id=&#34;whats-new-about-ctre&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s new about &lt;code&gt;CTRE&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;CTRE&lt;/code&gt; provides a new way to model the &lt;strong&gt;bursty timing&lt;/strong&gt; of the threshold crossings. In complex systems, &lt;em&gt;inter-event times&lt;/em&gt; are typically heavy-tailed&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, and thus highly heterogeneous (think income distributions). The bulk of inter-event times is short, corresponding to Bursts of events. A few very long inter-event times represent quiet periods.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://strakaps.github.io/MittagLeffleR/&#34;&gt;Mittag-Leffler distribution&lt;/a&gt; is universal for times between threshold crossings, see the &lt;a href=&#34;https://strakaps.github.io/bursty-POT/&#34;&gt;companion paper&lt;/a&gt;. &lt;code&gt;CTRE&lt;/code&gt; fits a Mittag-Leffler distribution to a range of threshold values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;flares_ctre &amp;lt;- thin(flares_ctre, k = 700)
par(mfrow = c(1,2))
MLestimates(flares_ctre, tail = 0.9, scale = 3E7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/post/ctre-package_files/figure-html/stability-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the threshold is varied from the 5th to the 700th largest magnitude (as you move right along the x-axes) and the parameters estimates are plotted on the y-axis. For both panels, towards the left (exceedances=5), only few data are available, and the estimate has a high variance. Towards the right (exceedances = 700), the threshold is low, and the Mittag-Leffler fit degrades (high bias). In between is where we want to read off our (hopefully stable) parameter estimates. The above plots are hence called “stability plots”.&lt;/p&gt;
&lt;dl&gt;
&lt;dt&gt;Important:&lt;/dt&gt;
&lt;dd&gt;&lt;p&gt;The scale parameter is proportional to &lt;span class=&#34;math inline&#34;&gt;\(k^{-1/\beta}\)&lt;/span&gt;. The parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is hence used to rescale the y-values of the right-hand plot, in order to make the estimates constant, and needs to estimated from the left-hand plot first.&lt;/p&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;div id=&#34;ctre-applied-to-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;code&gt;CTRE&lt;/code&gt; applied to data&lt;/h2&gt;
&lt;p&gt;For the solar flare data, we read off &lt;span class=&#34;math display&#34;&gt;\[
\beta \approx 0.9, \quad \sigma \approx 3 \times 10^7.
\]&lt;/span&gt; This means that the inter-arrival time &lt;span class=&#34;math inline&#34;&gt;\(T(k)\)&lt;/span&gt; for magnitudes as high as the &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-th magnitude is distributed as &lt;span class=&#34;math display&#34;&gt;\[
T(k) \sim {\rm ML}\left(0.9, \frac{3 \times 10^7 {\rm sec}}{k^{1/0.9}} \right).
\]&lt;/span&gt; For example, let’s assume the magnitude (peak rate) &lt;span class=&#34;math inline&#34;&gt;\(10,000\)&lt;/span&gt;. How many magnitudes exceed it?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
(flares_ctre %&amp;gt;% magnitudes() %&amp;gt;% sort(decreasing=TRUE) &amp;gt; 10000) %&amp;gt;% sum()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So substitute &lt;span class=&#34;math inline&#34;&gt;\(k = 24\)&lt;/span&gt;. The model thus predicts inter-arrival times drawn from the distribution &lt;span class=&#34;math display&#34;&gt;\[
{\rm ML}\left(0.9, 8.8\times 10^{5} {\rm sec} \right)
= {\rm ML}\left(0.9, 10 {\rm days} \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the plot of this distribution, on a logarithmic scale:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/post/ctre-package_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Compared to the Exponential distribution, the Mittag-Leffler distribution puts more probability mass on both tiny values (Bursts) and values in the tails (quiet periods).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-app&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny app&lt;/h2&gt;
&lt;p&gt;Running&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CTRE::runCTREshiny()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;opens a Shiny app that that lets you do the above manipulations via sliders. It is / should be loading below:&lt;/p&gt;
&lt;iframe src=&#34;https://strakaps.shinyapps.io/ctre-app/&#34; style=&#34;border: none; width: 800px; height: 900px&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-clustering-of-extremes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about “clustering of extremes”?&lt;/h2&gt;
&lt;p&gt;The main other appraoch in Extreme Value Theory which addresses irregular (non-Poissonian) threshold crossing times goes under the name “clustering of extremes”.&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; It assumes that the underlying process is stationary, and conflates each excursion of the process above the threshold as one cluster of events.&lt;/p&gt;
&lt;p&gt;CTREs and clustering of extremes both generalize the standard assumption of independent events, but in different directions. They are hence complementary, and it needs to be investigated how they should be compared meaningfully.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feedback&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Feedback&lt;/h2&gt;
&lt;p&gt;Do you think CTREs are a useful model? Please comment below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The &lt;a href=&#34;https://ntrs.nasa.gov/search.jsp?R=19920006736&#34;&gt;“complete Hard X Ray Burst Spectrometer event list”&lt;/a&gt; is a comprehensive reference for all measurements of the Hard X Ray Burst Spectrometer on NASA’s Solar Maximum Mission from the time of launch on Feb 14, 1980 to the end of the mission in Dec 1989. 12,776 events were detected, with the “vast majority being solar flares”. The list includes the start time, peak time, duration, and peak rate of each event. We have used “start time” as the variable for event times, and “peak rate” as the variable for event magnitudes.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See the Introduction of &lt;a href=&#34;http://iopscience.iop.org/article/10.1088/1367-2630/15/10/103023&#34;&gt;“Modelling bursty time series”&lt;/a&gt; by Vajna, Tóth &amp;amp; Kertész, for a list of complex systems with heavy-tailed inter-event times.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;See e.g. Chapter 3 in the textbook &lt;a href=&#34;https://books.google.com.ph/books/about/Extremes_and_Related_Properties_of_Rando.html?id=-ofTBwAAQBAJ&amp;amp;redir_esc=y&#34;&gt;&lt;em&gt;Extremes and Related Properties of Random Sequences and Processes&lt;/em&gt;&lt;/a&gt; by Leadbetter, Lindgren &amp;amp; Rootzén.&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Universal Algorithm for Continuous Time Random Walks Limit Distributions</title>
      <link>https://strakaps.github.io/publication/2018-universal/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/publication/2018-universal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Peaks Over Threshold for Bursty Time Series</title>
      <link>https://strakaps.github.io/publication/2018-bursty-pot/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +1000</pubDate>
      
      <guid>https://strakaps.github.io/publication/2018-bursty-pot/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fractional Diffusion where &#34;fractional&#34; varies in space</title>
      <link>https://strakaps.github.io/post/voffpe/</link>
      <pubDate>Mon, 18 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/voffpe/</guid>
      <description>


&lt;p&gt;The new paper (&lt;a href=&#34;http://arxiv.org/abs/1712.06767&#34;&gt;arxiv.org/abs/1712.06767&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Variable Order Fractional Fokker-Planck Equations derived from Continuous Time Random Walks&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;is currently under review.&lt;/p&gt;
&lt;p&gt;The variable order fractional Fokker-Planck equation (VOFFPE), in simple form&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, is &lt;span class=&#34;math display&#34;&gt;\[\frac{\partial}{\partial t} p(x,t) = \frac{\partial^2}{\partial x^2}
\underbrace{D_t^{1-\beta(x)}}_{\text{fractional derivative}}
p(x,t).\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you take out the fractional derivative operator, you get the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation&#34;&gt;&lt;strong&gt;Fokker-Planck equation&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;(FPE)&lt;/strong&gt;. It describes the spreading of probability via a random walk:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://strakaps.github.io/img/spreading.gif&#34; /&gt;

&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Adding in the &lt;a href=&#34;https://en.wikipedia.org/wiki/Fractional_calculus&#34;&gt;fractional derivative&lt;/a&gt; &lt;span class=&#34;math display&#34;&gt;\[D_t^{1-\beta} p(x,t) = \int_0^t p(x,t-s) \frac{s^{-\beta}}{\Gamma(1-\beta)}\,ds\]&lt;/span&gt; results in the “&lt;strong&gt;fractional FPE&lt;/strong&gt;”. The fractional derivative corresponds to longs rests of a walker, and slows down the spreading (making the dynamics “sub”diffusive).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Here the order &lt;span class=&#34;math inline&#34;&gt;\(1-\beta\)&lt;/span&gt; of the fractional derivative varies with the location &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; of the walker, hence the name VOFFPE (&lt;strong&gt;variable order fractional FPE&lt;/strong&gt;). In some regions, e.g. where the intracellular matrix of a cell is dense&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, the trappingness can be stronger, which is modelled by a smaller &lt;span class=&#34;math inline&#34;&gt;\(\beta(x)\)&lt;/span&gt; in that region.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The paper shows that the equation has a unique solution &lt;span class=&#34;math inline&#34;&gt;\(p(x,t)\)&lt;/span&gt;, and that this solution is the probability distribution of a Continuous Time Random Walk (CTRW)&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. CTRWs are random walks with random waiting times between jumps:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://strakaps.github.io/img/CTRW.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;The fractional derivative in time is tightly linked with &lt;a href=&#34;https://strakaps.github.io/post/mittag-leffler/&#34;&gt;Mittag-Leffler distributed&lt;/a&gt; waiting times.&lt;/p&gt;
&lt;p&gt;Of course CTRWs can be simulated, and &lt;span class=&#34;math inline&#34;&gt;\(p(x,t)\)&lt;/span&gt; may be computed via Monte Carlo simulations (or “&lt;a href=&#34;https://en.wikipedia.org/wiki/Single-particle_tracking&#34;&gt;particle tracking&lt;/a&gt;”). Thus a consequence of the paper is the mathematical certainty that the variable order FFPE (VOFFPE) can be solved by simulations.&lt;/p&gt;
&lt;p&gt;The variable order FFPE can also be solved directly, via clever numerical algorithms. The CTRW interpretation of the VOFFPE can be leveraged for such a clever numerical algorithm, as our friends over in applied maths have done with their &lt;a href=&#34;http://dx.doi.org/10.1016/j.jcp.2014.08.003&#34;&gt;Discrete Time Random Walk algorithm&lt;/a&gt;. This approach has several great benefits&lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, and now we know how to extend these benefits to the variable order setting (to be continued).&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Drift and diffusivity are left out here, but they may be any (Lipschitz-) continuous functions.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Wong et. al, &lt;a href=&#34;http://www.physics.emory.edu/faculty/weeks/lab/papers/Wong_04.pdf&#34;&gt;PRL 2004&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Strictly speaking, a scaling limit of a CTRW.&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;E.g. conservation of mass and non-negative solutions.&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Network Structure of General Practice in Australia</title>
      <link>https://strakaps.github.io/talk/asnac2018/</link>
      <pubDate>Mon, 27 Nov 2017 08:00:00 +1100</pubDate>
      
      <guid>https://strakaps.github.io/talk/asnac2018/</guid>
      <description>&lt;iframe src=&#34;https://docs.google.com/presentation/d/e/2PACX-1vSz9I5S64w7u_uYLHwS_y0NSmSANRsJDGtvO4Ul-uFo_mN5B9pBJXzWsgiLXgesCRxl597EzspV0hT6/embed?start=true&amp;loop=false&amp;delayms=3000&#34; frameborder=&#34;0&#34; width=&#34;800&#34; height=&#34;479&#34; allowfullscreen=&#34;true&#34; mozallowfullscreen=&#34;true&#34; webkitallowfullscreen=&#34;true&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>R package MittagLeffleR: Using the Mittag-Leffler distributions in R</title>
      <link>https://strakaps.github.io/post/mittag-leffler/</link>
      <pubDate>Mon, 26 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://strakaps.github.io/post/mittag-leffler/</guid>
      <description>


&lt;p&gt;Ricky Gill and I have created the R package MittagLeffleR, documented at &lt;a href=&#34;../../MittagLeffleR/index.html&#34;&gt;strakaps.github.io/MittagLeffleR/&lt;/a&gt;. It is available on &lt;a href=&#34;https://cran.r-project.org/web/packages/MittagLeffleR/index.html&#34;&gt;CRAN&lt;/a&gt; (and the latest development version on &lt;a href=&#34;https://github.com/strakaps/MittagLeffleR&#34;&gt;GitHub&lt;/a&gt;). MittagLeffleR calculates both Mittag-Leffler distributions; i.e. probability density, cumulative distribution function, quantile function and fast random variate generation. It was made possible by the &lt;a href=&#34;https://au.mathworks.com/matlabcentral/fileexchange/48154-the-mittag-leffler-function&#34;&gt;Laplace inversion algorithm&lt;/a&gt; for the Mittag-Leffler function by &lt;a href=&#34;https://twitter.com/rgarrappa&#34;&gt;Roberto Garrappa&lt;/a&gt;. Install it via&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;MittagLeffleR&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-two-mittag-leffler-distributions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Two Mittag-Leffler distributions&lt;/h2&gt;
&lt;p&gt;Various journal articles (including my own) write about “the” Mittag-Leffler distribution, but actually there are &lt;a href=&#34;https://en.wikipedia.org/wiki/Mittag-Leffler_distribution&#34;&gt;&lt;em&gt;two&lt;/em&gt; such families of distributions&lt;/a&gt;. Both have a shape parameter (&lt;code&gt;tail&lt;/code&gt;) taken from the interval &lt;span class=&#34;math inline&#34;&gt;\((0,1]\)&lt;/span&gt;, but the first distribution is &lt;em&gt;very heavy-tailed&lt;/em&gt; with infinite mean if &lt;code&gt;tail&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(\in (0,1)\)&lt;/span&gt; (it is equal to the exponential distribution if &lt;code&gt;tail&lt;/code&gt; &lt;span class=&#34;math inline&#34;&gt;\(=1\)&lt;/span&gt;) whereas the second distribution is light-tailed (has finite moments of all orders). The boolean variable &lt;code&gt;second.type&lt;/code&gt; switches between these two distributions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Examples&lt;/h2&gt;
&lt;div id=&#34;first-type-waiting-times-between-events&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;First type = waiting times between events&lt;/h3&gt;
&lt;p&gt;The first type Mittag-Leffler distribution commonly occurs for the &lt;strong&gt;inter-arrival times between events in complex systems&lt;/strong&gt; (e.g. earthquakes, see future post). Suppose that for an event to happen, several “tries” are needed, and that each try has a duration. The number &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; of tries until a success is geometrically distributed, and the &lt;em&gt;sum&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; durations is well approximated by a Mittag-Leffler (type I) distribution (this is the &lt;a href=&#34;https://github.com/strakaps/MittagLeffleR&#34;&gt;geometric stable&lt;/a&gt; property).&lt;/p&gt;
&lt;p&gt;As an example, we simulate &lt;code&gt;n=100&lt;/code&gt; Mittag-Leffler (type I) random variables, and estimate their tail and scale parameters via maximum likelihood:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MittagLeffleR)
y = rml(n = 100, tail = 0.9, scale = 2)
mlml &amp;lt;- function(X) {
  log_l &amp;lt;- function(theta) {
    #transform parameters so can do optimization unconstrained
    theta[1] &amp;lt;- 1/(1+exp(-theta[1]))
    theta[2] &amp;lt;- exp(theta[2])
    -sum(log(dml(X,theta[1],theta[2])))
  }
  ml_theta &amp;lt;- stats::optim(c(0.5,0.5), fn=log_l)$par
  #transform back
  ml_a &amp;lt;- 1/(1 + exp(-ml_theta[1]))
  ml_d &amp;lt;- exp(ml_theta[2])
  return(list(&amp;quot;tail&amp;quot; = ml_a, &amp;quot;scale&amp;quot; = ml_d))
}
mlml(y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $tail
## [1] 0.9390633
## 
## $scale
## [1] 1.600294&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;second-type-number-of-events&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Second type = number of events&lt;/h3&gt;
&lt;p&gt;The second type Mittag-Leffler distribution typically occurs for the random &lt;strong&gt;number of events in complex systems&lt;/strong&gt;. For instance, assume a random walk with heavy-tailed waiting times; this has become a widely adopted model for &lt;a href=&#34;https://en.wikipedia.org/wiki/Anomalous_diffusion&#34;&gt;anomalous diffusion&lt;/a&gt;. For late times, its number of steps &lt;a href=&#34;https://doi.org/10.1017/S002190020002043X&#34;&gt;can be well approximated by a Mittag-Leffler (type II) distribution&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The time-change of a random walk by the random number of steps is called &lt;em&gt;subordination&lt;/em&gt;; the random number of steps is approximated by the &lt;a href=&#34;https://stt.msu.edu/users/mcubed/hittingTime.pdf&#34;&gt;inverse stable subordinator&lt;/a&gt;. Below we calculate the probability density of a random walker with step size &lt;code&gt;dx=0.01&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the number of steps up to time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is stored in the vector &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the matrix &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; stores the probability distribution at the lattice sites at consecutive times&lt;/li&gt;
&lt;li&gt;the “subordination” happens in the matrix product &lt;code&gt;N %*% h&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MittagLeffleR)
tail &amp;lt;- 0.65
dx &amp;lt;- 0.01
x &amp;lt;- seq(-2,5,dx)
umax &amp;lt;- qml(p=0.99, tail=tail, scale=1, second.type=TRUE)
u &amp;lt;- seq(0.01,umax,dx)
h &amp;lt;- dml(x=u, tail=tail, scale=1, second.type=TRUE)
N &amp;lt;- outer(x,u,function(x,u){dnorm(x=x, mean=u, sd=sqrt(u))})
p &amp;lt;- N %*% h * dx
plot(x,p, type=&amp;#39;l&amp;#39;, main=&amp;quot;Fractional diffusion with drift at t=1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://strakaps.github.io/post/MittagLeffleR_files/figure-html/scary-shark-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bugs-and-issues&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bugs and Issues&lt;/h2&gt;
&lt;p&gt;Should you run into any bugs or issues, please &lt;a href=&#34;https://github.com/strakaps/MittagLeffleR/issues&#34;&gt;let us know&lt;/a&gt;. Thanks!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
